



# SAM2 è®°å½•

## sam2-docker å®‰è£…è®°å½•



æ­¤ç‰ˆæœ¬å…·å¤‡ä¸€ä¸ªå®Œæˆçš„dockeréƒ¨ç½²ï¼Œå¹¶èƒ½å¤Ÿå®ç°åŸºäºSAM2å’ŒSAM2.1çš„å…¨åˆ†å‰²å’ŒåŸºäºç‚¹çš„åˆ†å‰²

[Dicom-Vision/SAM2-Docker: ğŸ‘€ Segment Anything 2 + Docker ğŸ³](https://github.com/Dicom-Vision/SAM2-Docker)

```bash
docker pull peasant98/sam2:cuda-12.1

# mount this repo, which is assumed to be in the current directory
docker run -it -v /tmp/.X11-unix:/tmp/.X11-unix  -v `pwd`/SAM2-Docker:/home/user/SAM2-Docker -e DISPLAY=$DISPLAY --gpus all peasant98/sam2:cuda-12.1 bash

docker run -it -v /tmp/.X11-unix:/tmp/.X11-unix  -v /home/nyy/object-detect/SAM2-Docker:/home/user/SAM2-Docker -e DISPLAY=$DISPLAY --gpus all peasant98/sam2:cuda-12.1 bash
# in the container!
cd SAM2-Docker/
python3 examples/image_predictor.py

docker build -t sam2:v1 . 
docker compose -f docker-compose.yml up -d 


docker run -it --gpus all -e DISPLAY=$DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix  -v /home/nyy/object-detect/SAM2-Docker:/home/user/SAM2-Docker --shm-size 64g  --name sam2_nyy -p 5000:5000 --network host peasant98/sam2:cuda-12.1 bash
```

**åˆ›å»ºå…±äº«ç½‘ç»œ**ï¼š

```py
docker network create my_network_sam
```

**å¯åŠ¨åç«¯**

```
docker run --gpus all --network=my_network_sam -v /tmp/.X11-unix:/tmp/.X11-unix -itd -v /home/nyy/object-detect/SAM2-Docker:/home/user/SAM2-Docker --shm-size 64g  --name='sam_app' -p 10006:10006 peasant98/sam2:cuda-12.1 bash

pip install flask imageio[ffmpeg] nibabel APScheduler gunicorn pydicom -i https://pypi.tuna.tsinghua.edu.cn/simple/

sudo apt-get install net-tools
```

**å¯åŠ¨ Nginx å¹¶åŠ å…¥åŒä¸€ç½‘ç»œ**ï¼š

```
proxy_pass http://sam_app:10006;  # é€šè¿‡Dockerç½‘ç»œç›´æ¥è§£æå®¹å™¨å

docker run -itd --name='sam_sever' -p 80:80 --network=my_network_sam -v /home/nyy/object-detect/SAM2-Docker/nginx.conf:/etc/nginx/conf.d/default.conf nginx:latest
```

## sam2-docker æ€»ç»“

1ã€ä½¿ç”¨sam2-docker æœ€é‡è¦å®ç°ï¼špeasant98/sam2:cuda-12.1ï¼ˆåŸºäºnvidia/cuda:12.1.0-devel-ubuntu20.04ï¼‰æ„å»ºç¯å¢ƒSAM2ç¯å¢ƒï¼ˆå‚è€ƒâ€œå¯åŠ¨åç«¯â€éƒ¨åˆ†ï¼‰ã€‚

2ã€å°è¯•ä½¿ç”¨Nginxä»£ç†dockeræœåŠ¡ï¼Œå…·ä½“æ­¥éª¤å¯å‚è€ƒä»¥ä¸Šæµç¨‹

## å…¨åˆ†å‰²åˆ†å‰²ä»£ç ï¼štest_sam2.1.py

<details>
<summary>ç‚¹å‡»å±•å¼€/æŠ˜å ä»£ç /test_sam2.1.py</summary>

```python
import numpy as np
import torch
from PIL import Image
import cv2
from sam2.build_sam import build_sam2
from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator
import time
 
# Enable CUDA optimizations
if torch.cuda.is_available():
    torch.autocast(device_type="cuda", dtype=torch.float16).__enter__()
 
    if torch.cuda.get_device_properties(0).major >= 8:
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
 
def apply_color_mask(image, mask, color, color_dark=0.5):
    """Apply colored mask to the image."""
    for c in range(3):
        image[:, :, c] = np.where(mask == 1, image[:, :, c] * (1 - color_dark) + color_dark * color[c], image[:, :, c])
    return image
 
def init_sam(sam2_checkpoint = "checkpoints/sam2_hiera_large.pt", model_cfg = "sam2_hiera_l.yaml"):

    # Load SAM2 Model
    sam2 = build_sam2(model_cfg, sam2_checkpoint, device='cuda', apply_postprocessing=False)
    # sam2.to(device="cuda" if torch.cuda.is_available() else "cpu")
 
    # Initialize mask generator
    mask_generator = SAM2AutomaticMaskGenerator(
            sam2,
            points_per_side=32,  # æ§åˆ¶åˆ†å‰²å¯†åº¦
            pred_iou_thresh=0.86,  # è¿‡æ»¤ä½è´¨é‡åˆ†å‰²
            stability_score_thresh=0.92,  # è¿‡æ»¤ä¸ç¨³å®šåˆ†å‰²
            crop_n_layers=1,  # ä½¿ç”¨è£å‰ªå±‚æé«˜å°ç‰©ä½“æ£€æµ‹
            crop_n_points_downscale_factor=2,
            min_mask_region_area=100  # è¿‡æ»¤å°åŒºåŸŸ
        )
    # sam = sam_model_registry[model_type](checkpoint=model_path)
    # sam.to(device="cuda" if torch.cuda.is_available() else "cpu")
    # mask_generator = SamAutomaticMaskGenerator(
    #     sam,
    #     points_per_side=32,  # æ§åˆ¶åˆ†å‰²å¯†åº¦
    #     pred_iou_thresh=0.86,  # è¿‡æ»¤ä½è´¨é‡åˆ†å‰²
    #     stability_score_thresh=0.92,  # è¿‡æ»¤ä¸ç¨³å®šåˆ†å‰²
    #     crop_n_layers=1,  # ä½¿ç”¨è£å‰ªå±‚æé«˜å°ç‰©ä½“æ£€æµ‹
    #     crop_n_points_downscale_factor=2,
    #     min_mask_region_area=100  # è¿‡æ»¤å°åŒºåŸŸ
    # )
    return mask_generator


def preprocess_image(image_path, target_size=1024):
    image = cv2.imread(image_path)
    h, w = image.shape[:2]
    scale = target_size / max(h, w)
    image = cv2.resize(image, (int(w*scale), int(h*scale)))
    return image

def enhance_foreground(foreground, background_strength=0.2):
    """å‰æ™¯å¢å¼ºå¤„ç†"""
    # åˆ›å»ºçº¯é»‘èƒŒæ™¯
    enhanced = foreground.copy()
    
    # å¯é€‰ï¼šç»™èƒŒæ™¯æ·»åŠ è½»å¾®çš„åŸå›¾æ¨¡ç³Šï¼ˆä¿æŒç©ºé—´æ„Ÿï¼‰
    if background_strength > 0:
        background = cv2.GaussianBlur(foreground, (25, 25), 0)
        mask = (foreground.sum(axis=2) == 0).astype(np.uint8)
        enhanced = cv2.addWeighted(
            enhanced, 1, 
            background, background_strength, 
            0
        )
    
    # å¢å¼ºå‰æ™¯å¯¹æ¯”åº¦
    enhanced = cv2.cvtColor(enhanced, cv2.COLOR_BGR2LAB)
    l, a, b = cv2.split(enhanced)
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
    l = clahe.apply(l)
    enhanced = cv2.merge((l, a, b))
    enhanced = cv2.cvtColor(enhanced, cv2.COLOR_LAB2BGR)
    
    return enhanced

def main(img_path,output_path):
    sam2_checkpoint = "checkpoints/sam2_hiera_large.pt"
    model_cfg = "sam2_hiera_l.yaml"

    image = preprocess_image(img_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) 

    mask_generator = init_sam()
 
    # Generate masks
    start = time.time()
    start_time = time.perf_counter()
    masks = mask_generator.generate(image)
    print(f"sam2 infer: {time.time() - start:.3f}s")

    inference_time_ms = (time.perf_counter() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
    print(f"SAM æ¨ç†æ—¶é—´: {inference_time_ms:.2f} ms")
 
    # Apply masks and save result
    image_select = image.copy()
    for mask_info in masks:
        color = tuple(np.random.randint(0, 256, 3).tolist())
        selected_mask = mask_info['segmentation']
        image_select = apply_color_mask(image_select, selected_mask, color)
 
    cv2.imwrite(output_path, image_select)
    print(f"Result saved to {output_path}")
    
    # # åˆ›å»ºç™½è‰²èƒŒæ™¯ (BGRæ ¼å¼)
    # foreground = np.zeros_like(image)
    # # foreground = np.full_like(image, [255, 255, 255])  # BGRç™½è‰²
    
    # # åˆå¹¶æ‰€æœ‰åˆ†å‰²åŒºåŸŸ
    # for ann in masks:
    #     mask = ann['segmentation']
    #     # foreground[mask] = image[mask]  # åªä¿ç•™å‰æ™¯åŒºåŸŸ
    #     for c in range(3):
    #         foreground[:,:,c][mask] = image[:,:,c][mask]

    # # 4. å‰æ™¯å¢å¼º
    # enhanced = enhance_foreground(foreground)

    # # 5. ä¿å­˜ç»“æœ
    # cv2.imwrite(output_path, enhanced)
    # print(f"ç»“æœå·²ä¿å­˜è‡³ {output_path}")
    
if __name__ == "__main__":
    # main("img_375.jpg", "img_375_FG.jpg")
    # main("test_02.jpg", "../result/test_02_sam2_.jpg")
    main("test_01.jpg", "../result/test_01_sam2.jpg")
    # main("test_03.jpg", "test_03_FG.jpg")

```

</details>

## å…¨åˆ†å‰²åˆ†å‰²ä»£ç ï¼štest_sam2.py

<details>
<summary>ç‚¹å‡»å±•å¼€/æŠ˜å ä»£ç /test_sam2.py</summary>


```python

import time
import numpy as np
import torch
import cv2
import matplotlib.pyplot as plt
from segment_anything import sam_model_registry, SamAutomaticMaskGenerator

def apply_color_mask(image, mask, color, color_dark=0.5):
    """Apply colored mask to the image."""
    for c in range(3):
        image[:, :, c] = np.where(mask == 1, image[:, :, c] * (1 - color_dark) + color_dark * color[c], image[:, :, c])
    return image
# åˆå§‹åŒ–æ¨¡å‹
def init_sam(model_path="checkpoints/sam_vit_l_0b3195.pth", model_type="vit_l"):
    sam = sam_model_registry[model_type](checkpoint=model_path)
    sam.to(device="cuda" if torch.cuda.is_available() else "cpu")
    mask_generator = SamAutomaticMaskGenerator(
        sam,
        points_per_side=32,  # æ§åˆ¶åˆ†å‰²å¯†åº¦
        pred_iou_thresh=0.86,  # è¿‡æ»¤ä½è´¨é‡åˆ†å‰²
        stability_score_thresh=0.92,  # è¿‡æ»¤ä¸ç¨³å®šåˆ†å‰²
        crop_n_layers=1,  # ä½¿ç”¨è£å‰ªå±‚æé«˜å°ç‰©ä½“æ£€æµ‹
        crop_n_points_downscale_factor=2,
        min_mask_region_area=100  # è¿‡æ»¤å°åŒºåŸŸ
    )
    return mask_generator

def preprocess_image(image_path, target_size=1024):
    image = cv2.imread(image_path)
    h, w = image.shape[:2]
    scale = target_size / max(h, w)
    image = cv2.resize(image, (int(w*scale), int(h*scale)))
    return image

def enhance_foreground(foreground, background_strength=0.2):
    """å‰æ™¯å¢å¼ºå¤„ç†"""
    # åˆ›å»ºçº¯é»‘èƒŒæ™¯
    enhanced = foreground.copy()
    
    # å¯é€‰ï¼šç»™èƒŒæ™¯æ·»åŠ è½»å¾®çš„åŸå›¾æ¨¡ç³Šï¼ˆä¿æŒç©ºé—´æ„Ÿï¼‰
    if background_strength > 0:
        background = cv2.GaussianBlur(foreground, (25, 25), 0)
        mask = (foreground.sum(axis=2) == 0).astype(np.uint8)
        enhanced = cv2.addWeighted(
            enhanced, 1, 
            background, background_strength, 
            0
        )
    
    # å¢å¼ºå‰æ™¯å¯¹æ¯”åº¦
    enhanced = cv2.cvtColor(enhanced, cv2.COLOR_BGR2LAB)
    l, a, b = cv2.split(enhanced)
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
    l = clahe.apply(l)
    enhanced = cv2.merge((l, a, b))
    enhanced = cv2.cvtColor(enhanced, cv2.COLOR_LAB2BGR)
    
    return enhanced


# åˆ†å‰²å›¾åƒä¸­çš„æ‰€æœ‰ç‰©ä½“å¹¶ä¿å­˜å‰æ™¯
def segment_everything(image_path, output_path="FG.jpg"):
    # è¯»å–å›¾åƒ
    # image = cv2.imread(image_path)
    image = preprocess_image(image_path)

    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    
    
    # ç”Ÿæˆæ©ç 
    mask_generator = init_sam()
    start_time = time.perf_counter()

    masks = mask_generator.generate(image)

    inference_time_ms = (time.perf_counter() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
    print(f"SAM æ¨ç†æ—¶é—´: {inference_time_ms:.2f} ms")

    # Apply masks and save result
    image_select = image.copy()
    for mask_info in masks:
        color = tuple(np.random.randint(0, 256, 3).tolist())
        selected_mask = mask_info['segmentation']
        image_select = apply_color_mask(image_select, selected_mask, color)
 
    cv2.imwrite(output_path, image_select)
    print(f"Result saved to {output_path}")
    
    # åˆ›å»ºå‰æ™¯å›¾åƒ
    # foreground = np.zeros_like(image)
    # åˆ›å»ºç™½è‰²èƒŒæ™¯ (BGRæ ¼å¼)
    # foreground = np.full_like(image, [0, 0, 255])  # BGRç™½è‰²
    # # foreground = np.full_like(image, [255, 255, 255])  # BGRç™½è‰²
    # foreground_1 = np.full_like(image, [255, 255, 255])  # BGRç™½è‰²
    # foreground_2 = np.full_like(image, [255, 255, 255])  # BGRç™½è‰²    
    
    # # åˆå¹¶æ‰€æœ‰åˆ†å‰²åŒºåŸŸ
    # for ann in masks:
    #     mask = ann['segmentation']
    #     # foreground[mask] = image[mask]  # åªä¿ç•™å‰æ™¯åŒºåŸŸ
    #     for c in range(3):
    #         foreground[:,:,c][mask] = image[:,:,c][mask]

    # # 4. å‰æ™¯å¢å¼º
    # enhanced = enhance_foreground(foreground)

    # # 5. ä¿å­˜ç»“æœ
    # cv2.imwrite(output_path, enhanced)
    # print(f"ç»“æœå·²ä¿å­˜è‡³ {output_path}")
    
    return masks

# ä½¿ç”¨ç¤ºä¾‹
segment_everything("test_01.jpg", "../result/test_01_FG_1.jpg")
# segment_everything("test_03.jpg", "test_03_FG.jpg")
```
</details>

### å›¾ç‰‡åˆ†å‰²ç»“æœï¼š

<p align = "center"> 
<img  src="./SAM/test_02_FG.jpg" width="400" />
<img  src="./SAM/2024-07-19 16_11_04_567_FG.jpg" width="400" />
</p>

### è§†é¢‘è§†é¢‘ç»“æœï¼š

[ç‚¹å‡»è§‚çœ‹è§†é¢‘](./CVAT/people-sam2.mp4)

[ç‚¹å‡»è§‚çœ‹è§†é¢‘](street_sam2.mp4)

![](./CVAT/people-sam2.mp4)

<video width="320" height="240" controls>
    <source src="street_sam2.mp4" type="video/mp4">
  ä½ çš„æµè§ˆå™¨ä¸æ”¯æŒ HTML5 è§†é¢‘æ ‡ç­¾ã€‚
</video>



<video src="people-sam2.mp4" autoplay="true" controls="controls" width="800" height="600">
</video>



# sam2 å®‰è£…è®°å½•

[facebookresearch/sam2 at sam2.1](https://github.com/facebookresearch/sam2/tree/sam2.1)

è§†é¢‘

https://sam2.metademolab.com/demo

å›¾ç‰‡ï¼š

[Segment Anything | Meta AI](https://segment-anything.com/demo#)



